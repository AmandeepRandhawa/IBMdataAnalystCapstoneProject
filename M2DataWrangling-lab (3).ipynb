{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "90ac80ca41fde58b95f02cc0cfc0a0f2ed3115615ffe1bdfdfc10fd5ffeef9a4"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Data Wrangling Lab**\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Estimated time needed: **45 to 60** minutes\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this assignment you will be performing data wrangling.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Objectives\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this lab you will perform the following:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "-   Identify duplicate values in the dataset.\n\n-   Remove duplicate values from the dataset.\n\n-   Identify missing values in the dataset.\n\n-   Impute the missing values in the dataset.\n\n-   Normalize data in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hands on Lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Import pandas module.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-1-7dd3504c366f>:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "Load the dataset into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Read Data</h2>\n<p>\nWe utilize the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The functions below will download the dataset into your browser:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from pyodide.http import pyfetch\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "To obtain the dataset, utilize the download() function as defined above:  \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "await download(file_path, \"m1_survey_data.csv\")\nfile_name=\"m1_survey_data.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "Utilize the Pandas method read_csv() to load the data into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(file_name)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "> Note: This version of the lab is working on JupyterLite, which requires the dataset to be downloaded to the interface.While working on the downloaded version of this notebook on their local machines(Jupyter Anaconda), the learners can simply **skip the steps above,** and simply use the URL directly in the `pandas.read_csv()` function. You can uncomment and run the statements in the cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/LargeData/m1_survey_data.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Finding duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this section you will identify duplicate values in the dataset.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Find how many duplicate rows exist in the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes \n\n\n# Identify duplicate rows\nduplicate_rows = df.duplicated()\n\n# Count the number of duplicate rows\nnum_duplicate_rows = duplicate_rows.sum()\n\n# Print the number of duplicate rows\nprint(f\"There are {num_duplicate_rows} duplicate rows in the dataset.\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 154 duplicate rows in the dataset.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "## Removing duplicates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Remove the duplicate rows from the dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n\n# Print the number of rows before removing duplicates\nprint(f\"Number of rows before removing duplicates: {len(df)}\")\n\n# Remove duplicate rows from the DataFrame\ndf_unique = df.drop_duplicates()\n\n# Print the number of rows after removing duplicates\nprint(f\"Number of rows after removing duplicates: {len(df_unique)}\")\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of rows before removing duplicates: 11552\nNumber of rows after removing duplicates: 11398\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "Verify if duplicates were actually dropped.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n\n# Print the number of rows before removing duplicates\ninitial_row_count = len(df)\nprint(f\"Number of rows before removing duplicates: {initial_row_count}\")\n\n# Remove duplicate rows from the DataFrame\ndf_unique = df.drop_duplicates()\n\n# Print the number of rows after removing duplicates\nfinal_row_count = len(df_unique)\nprint(f\"Number of rows after removing duplicates: {final_row_count}\")\n\n# Check if any duplicates remain in the cleaned DataFrame\nremaining_duplicates = df_unique.duplicated().sum()\nprint(f\"Number of remaining duplicate rows: {remaining_duplicates}\")\n\n# Verify if duplicates were dropped\nif initial_row_count > final_row_count and remaining_duplicates == 0:\n    print(\"Duplicates were successfully removed.\")\nelse:\n    print(\"There are still duplicates in the dataset, or no duplicates were found initially.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of rows before removing duplicates: 11552\nNumber of rows after removing duplicates: 11398\nNumber of remaining duplicate rows: 0\nDuplicates were successfully removed.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "## Finding Missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the missing values for all columns.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Find the number of missing values in each column\nmissing_values = df.isnull().sum()\n\n# Print the number of missing values for each column\nprint(\"Missing values in each column:\")\nprint(missing_values)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Missing values in each column:\nRespondent        0\nMainBranch        0\nHobbyist          0\nOpenSourcer       0\nOpenSource       81\n               ... \nSexuality       547\nEthnicity       683\nDependents      144\nSurveyLength     19\nSurveyEase       14\nLength: 85, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "Find out how many rows are missing in the column 'WorkLoc'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes \n\n\n# Count the number of missing values in the 'WorkLoc' column\nmissing_workloc_count = df['WorkLoc'].isnull().sum()\n\n# Print the number of missing values\nprint(f\"The 'WorkLoc' column has {missing_workloc_count} missing values.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The 'WorkLoc' column has 32 missing values.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": "## Imputing missing values\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find the  value counts for the column WorkLoc.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes here\n\n# Get the value counts for the 'WorkLoc' column\nworkloc_value_counts = df['WorkLoc'].value_counts(dropna=False)\n\n# Print the value counts\nprint(\"Value counts for the 'WorkLoc' column:\")\nprint(workloc_value_counts)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Value counts for the 'WorkLoc' column:\nWorkLoc\nOffice                                            6905\nHome                                              3638\nOther place, such as a coworking space or cafe     977\nNaN                                                 32\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": "Identify the value that is most frequent (majority) in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#make a note of the majority value here, for future reference\n\n# Get the value counts for the 'WorkLoc' column\nworkloc_value_counts = df['WorkLoc'].value_counts()\n\n# Identify the most frequent value (majority)\nmost_frequent_value = workloc_value_counts.idxmax()\nmost_frequent_count = workloc_value_counts.max()\n\n# Print the most frequent value and its count\nprint(f\"The most frequent value in the 'WorkLoc' column is '{most_frequent_value}' with {most_frequent_count} occurrences.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The most frequent value in the 'WorkLoc' column is 'Office' with 6905 occurrences.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": "Impute (replace) all the empty rows in the column WorkLoc with the value that you have identified as majority.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n\n# Identify duplicate values in the 'Respondent' column\nduplicates = df['Respondent'].duplicated()\n\n# Count the number of duplicate entries\nnum_duplicates = duplicates.sum()\n\n# Print the number of duplicate values\nprint(f\"There are {num_duplicates} duplicate values in the 'Respondent' column.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 154 duplicate values in the 'Respondent' column.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "\n\n# Identify the most frequent value in the 'WorkLoc' column\nmost_frequent_value = df['WorkLoc'].value_counts().idxmax()\n\n# Impute (replace) all empty rows in the 'WorkLoc' column with the most frequent value\ndf['WorkLoc'].fillna(most_frequent_value, inplace=True)\n\n# Verify that the missing values have been replaced\nmissing_after_imputation = df['WorkLoc'].isnull().sum()\nprint(f\"Number of missing values in 'WorkLoc' after imputation: {missing_after_imputation}\")\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-14-c14e939525f4>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['WorkLoc'].fillna(most_frequent_value, inplace=True)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Number of missing values in 'WorkLoc' after imputation: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": "After imputation there should ideally not be any empty rows in the WorkLoc column.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Verify if imputing was successful.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# your code goes he\n\n# Identify the most frequent value in the 'WorkLoc' column\nmost_frequent_value = df['WorkLoc'].value_counts().idxmax()\n\n# Impute (replace) all empty rows in the 'WorkLoc' column with the most frequent value\ndf['WorkLoc'].fillna(most_frequent_value, inplace=True)\n\n# Verify if the imputation was successful\n# 1. Check if there are any remaining missing values in 'WorkLoc'\nmissing_after_imputation = df['WorkLoc'].isnull().sum()\n\n# 2. Print the number of missing values after imputation\nprint(f\"Number of missing values in 'WorkLoc' after imputation: {missing_after_imputation}\")\n\n# Optional: Check if the number of missing values has been reduced as expected\n# Compare with the initial count of missing values (if you have that information)\ninitial_missing_values = df['WorkLoc'].isnull().sum()\nprint(f\"Number of missing values before imputation: {initial_missing_values}\")\n\n# Additional Verification: Check the number of replacements (i.e., rows that had NaN and were replaced)\nimputed_rows_count = initial_missing_values - missing_after_imputation\nprint(f\"Number of rows imputed: {imputed_rows_count}\")\n\n# Optional: Save the updated dataset to a new CSV file\ndf.to_csv('updated_survey_data.csv', index=False)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of missing values in 'WorkLoc' after imputation: 0\nNumber of missing values before imputation: 0\nNumber of rows imputed: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": "## Normalizing data\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are two columns in the dataset that talk about compensation.\n\nOne is \"CompFreq\". This column shows how often a developer is paid (Yearly, Monthly, Weekly).\n\nThe other is \"CompTotal\". This column talks about how much the developer is paid per Year, Month, or Week depending upon his/her \"CompFreq\". \n\nThis makes it difficult to compare the total compensation of the developers.\n\nIn this section you will create a new column called 'NormalizedAnnualCompensation' which contains the 'Annual Compensation' irrespective of the 'CompFreq'.\n\nOnce this column is ready, it makes comparison of salaries easy.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "List out the various categories in the column 'CompFreq'\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\n# Create a function to normalize compensation based on frequency\ndef normalize_compensation(row):\n    if row['CompFreq'] == 'Yearly':\n        return row['CompTotal']\n    elif row['CompFreq'] == 'Monthly':\n        return row['CompTotal'] * 12\n    elif row['CompFreq'] == 'Weekly':\n        return row['CompTotal'] * 52\n    else:\n        return None  # Handle unexpected values or missing data\n\n# Apply the function to create the 'NormalizedAnnualCompensation' column\ndf['NormalizedAnnualCompensation'] = df.apply(normalize_compensation, axis=1)\n\n# Verify the new column\nprint(df[['CompFreq', 'CompTotal', 'NormalizedAnnualCompensation']].head())\n\n# Optional: Save the updated dataset to a new CSV file\ndf.to_csv('updated_survey_data_with_normalized_compensation.csv', index=False)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  CompFreq  CompTotal  NormalizedAnnualCompensation\n0   Yearly    61000.0                       61000.0\n1   Yearly   138000.0                      138000.0\n2   Yearly    90000.0                       90000.0\n3  Monthly    29000.0                      348000.0\n4   Yearly    90000.0                       90000.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Count missing values in the 'EdLevel' column after removing duplicates\nmissing_edlevel_count = df['EdLevel'].isnull().sum()\n\n# Print the number of missing values in 'EdLevel'\nprint(f\"Number of blank (missing) rows in the 'EdLevel' column after removing duplicates: {missing_edlevel_count}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of blank (missing) rows in the 'EdLevel' column after removing duplicates: 112\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": "Create a new column named 'NormalizedAnnualCompensation'. Use the hint given below if needed.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\n# Get the value counts for the 'Employment' column\nemployment_value_counts = df['Employment'].value_counts()\n\n# Identify the majority category\nmajority_category = employment_value_counts.idxmax()\nmajority_count = employment_value_counts.max()\n\n# Print the majority category and its count\nprint(f\"The majority category in the 'Employment' column is '{majority_category}' with {majority_count} occurrences.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The majority category in the 'Employment' column is 'Employed full-time' with 10968 occurrences.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": "\n# Get the value counts for the 'UndergradMajor' column\nundergrad_major_counts = df['UndergradMajor'].value_counts()\n\n# Identify the category with the minimum number of rows\nmin_category = undergrad_major_counts.idxmin()\nmin_count = undergrad_major_counts.min()\n\n# Print the category with the minimum count and its count\nprint(f\"The category with the minimum number of rows in 'UndergradMajor' is '{min_category}' with {min_count} occurrences.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The category with the minimum number of rows in 'UndergradMajor' is 'A health science (ex. nursing, pharmacy, radiology)' with 24 occurrences.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": "Double click to see the **Hint**.\n\n<!--\n\nUse the below logic to arrive at the values for the column NormalizedAnnualCompensation.\n\nIf the CompFreq is Yearly then use the exising value in CompTotal\nIf the CompFreq is Monthly then multiply the value in CompTotal with 12 (months in an year)\nIf the CompFreq is Weekly then multiply the value in CompTotal with 52 (weeks in an year)\n\n-->\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\n# Calculate the mean or median of the 'ConvertedComp' column\nmean_value = df['ConvertedComp'].mean()\nmedian_value = df['ConvertedComp'].median()\n\n# Impute missing values with mean or median\ndf['ConvertedComp'].fillna(median_value, inplace=True)  # Using median for robustness\n\n# Verify imputation\nmissing_after_imputation = df['ConvertedComp'].isnull().sum()\nprint(f\"Number of missing values in 'ConvertedComp' after imputation: {missing_after_imputation}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-20-236194142b16>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['ConvertedComp'].fillna(median_value, inplace=True)  # Using median for robustness\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Number of missing values in 'ConvertedComp' after imputation: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": "from sklearn.impute import SimpleImputer\n\n# Create an imputer object with the strategy of mean\nimputer = SimpleImputer(strategy='median')\n\n# Fit and transform the 'ConvertedComp' column\ndf[['ConvertedComp']] = imputer.fit_transform(df[['ConvertedComp']])\n\n# Verify imputation\nmissing_after_imputation = df['ConvertedComp'].isnull().sum()\nprint(f\"Number of missing values in 'ConvertedComp' after imputation: {missing_after_imputation}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of missing values in 'ConvertedComp' after imputation: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Function to normalize compensation based on frequency\ndef normalize_annual_compensation(row):\n    if row['CompFreq'] == 'Yearly':\n        return row['CompTotal']\n    elif row['CompFreq'] == 'Monthly':\n        return row['CompTotal'] * 12\n    elif row['CompFreq'] == 'Weekly':\n        return row['CompTotal'] * 52\n    else:\n        return None  # Handle unexpected values or missing data\n\n# Create the 'NormalizedAnnualCompensation' column\ndf['NormalizedAnnualCompensation'] = df.apply(normalize_annual_compensation, axis=1)\n\n# Verify the new column\nprint(df[['CompFreq', 'CompTotal', 'NormalizedAnnualCompensation']].head())\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  CompFreq  CompTotal  NormalizedAnnualCompensation\n0   Yearly    61000.0                       61000.0\n1   Yearly   138000.0                      138000.0\n2   Yearly    90000.0                       90000.0\n3  Monthly    29000.0                      348000.0\n4   Yearly    90000.0                       90000.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": "\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Count the number of respondents paid yearly\nyearly_payers_count = df[df['CompFreq'] == 'Yearly'].shape[0]\n\n# Print the number of respondents paid yearly\nprint(f\"Number of respondents being paid yearly: {yearly_payers_count}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of respondents being paid yearly: 6073\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": "\n\n# Create the 'NormalizedAnnualCompensation' column if it doesn't exist\n# Define a function to normalize compensation based on frequency\ndef normalize_compensation(row):\n    if row['CompFreq'] == 'Yearly':\n        return row['CompTotal']\n    elif row['CompFreq'] == 'Monthly':\n        return row['CompTotal'] * 12\n    elif row['CompFreq'] == 'Weekly':\n        return row['CompTotal'] * 52\n    else:\n        return None  # Handle unexpected values or missing data\n\n# Apply the function to create the 'NormalizedAnnualCompensation' column\ndf['NormalizedAnnualCompensation'] = df.apply(normalize_compensation, axis=1)\n\n# Calculate the median of the 'NormalizedAnnualCompensation' column\nmedian_compensation = df['NormalizedAnnualCompensation'].median()\n\n# Print the median value\nprint(f\"The median NormalizedAnnualCompensation is {median_compensation}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The median NormalizedAnnualCompensation is 100000.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": "\n\n# Check for unique values in the 'CompFreq' column\nunique_comp_freq_values = df['CompFreq'].nunique()\n\n# Print the number of unique values\nprint(f\"Number of unique values in 'CompFreq': {unique_comp_freq_values}\")\n\n# Optional: List the unique values\nunique_values_list = df['CompFreq'].unique()\nprint(f\"Unique values in 'CompFreq': {unique_values_list}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of unique values in 'CompFreq': 3\nUnique values in 'CompFreq': ['Yearly' 'Monthly' 'Weekly' nan]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": "\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Filter and count respondents with 'CompFreq' as 'Yearly'\nyearly_payers_count = df[df['CompFreq'] == 'Yearly'].shape[0]\n\n# Print the number of respondents paid yearly\nprint(f\"Number of respondents being paid yearly: {yearly_payers_count}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of respondents being paid yearly: 6073\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ramesh Sannareddy\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Rav Ahuja\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " Copyright © 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDA0321ENSkillsNetwork928-2022-01-01&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBM-DA0321EN-SkillsNetwork-21426264&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--| Date (YYYY-MM-DD) | Version | Changed By        | Change Description                 |\n| ----------------- | ------- | ----------------- | ---------------------------------- |\n| 2020-10-17        | 0.1     | Ramesh Sannareddy | Created initial version of the lab |--!>\n",
      "metadata": {}
    }
  ]
}